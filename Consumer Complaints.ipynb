{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEAMLESS COMPLAINTS RESOLUTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='blob.jpeg', width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fast-paced and interconnected world of modern business, customer feedback plays a crucial role in shaping the success of any organization. Companies across various industries receive a multitude of complaints from their customers daily. Effectively handling these complaints and routing them to the appropriate departments for timely resolution is essential for maintaining customer satisfaction and streamlining internal operations.\n",
    "\n",
    "To address this challenge, I am embarking on an innovative project to develop a cutting-edge Natural Language Processing (NLP) model. The primary objective of this model is to read and comprehend natural language in customer complaints, enabling us to automatically route each complaint to the right department or team for efficient and tailored resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdUvEaZgSYpo"
   },
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMUKjjcaRpLv"
   },
   "outputs": [],
   "source": [
    "#Importing libraries for data cleaning, processing and visualisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.express  as px\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "#Libraries for Natural Language processing\n",
    "import nltk\n",
    "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "import urllib\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Libraries for data engineering and modeling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVC\n",
    "Random_state=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('complaints_processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "- The dataframe has three features namely unnamed: 0, product and narrative. It is important to also note that the unnamed column consist of the data that wont be necessary for the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More infomation on the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking number of observations and datatype')\n",
    "print()\n",
    "print(df.info())\n",
    "print()\n",
    "print('Checking for nulls')\n",
    "print()\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "    \n",
    "- The data consist of 162421 entries, with the narrative column having 10 missing observations.\n",
    "- The two features namely Product and narrative datatypes are object and the third one datatype is integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pie chart showing the distribution of consumer complaints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for the 'product' column\n",
    "product_counts = df['product'].value_counts()\n",
    "\n",
    "# Set custom shades of blue\n",
    "colors = ['#1f77b4', '#3581b8', '#4d8fbf', '#63a8c6', '#79b0cd']\n",
    "\n",
    "# Plotting the pie chart\n",
    "plt.figure(figsize=(8, 8))  # Set the figure size (optional)\n",
    "wedges, texts, _ = plt.pie(product_counts.values, labels=product_counts.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "\n",
    "# Set the text in the center of each slice with the exact count\n",
    "for text in texts:\n",
    "    text.set_color('white')  # Setting text color to white\n",
    "    text.set_fontsize(12)  # Setting text font size\n",
    "    text.set_fontweight('bold')  # Setting text font weight\n",
    "\n",
    "# Adding the count as legend on the left side\n",
    "legend_labels = [f'{product}: {count}' for product, count in zip(product_counts.index, product_counts.values)]\n",
    "plt.legend(wedges, legend_labels, title='Product', loc='center left', bbox_to_anchor=(-0.4, 0.5))\n",
    "\n",
    "# Title\n",
    "plt.title('Consumer Complaints Distribution')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "- Consumer complaints are directed to five departments namely: credit reporting, debt collection, mortgages & loans, credit cards and retail banking. \n",
    "- Most of the consumer complaints are directed to credit reporting departing, it accounts to more than 50% of the consumer complaints. The class imbalance will be handled during feature engineering to ensure the robustness of the model. \n",
    "- Credit reporting is a department that gather and maintain information about individuals' credit activities and creditworthiness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bar graph showing the distribution of word count for each deprtment consumer complaint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the narrative column to string\n",
    "df['narrative']=df['narrative'].astype(str)\n",
    "\n",
    "#Creating a number of Words feature\n",
    "df['num_words']=df['narrative'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "\n",
    "# Creating a 3x2 grid of subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "# Plot histogram for  'credict_reporting'\n",
    "sns.histplot(df[df['product'] == 'credit_reporting']['num_words'], ax=axes[0, 0], color='blue')\n",
    "axes[0, 0].set_title('Department: Credit Reporting')\n",
    "\n",
    "# Plotting histogram for 'debt_collection'\n",
    "sns.histplot(df[df['product'] == 'debt_collection']['num_words'], ax=axes[0, 1], color='red')\n",
    "axes[0, 1].set_title('Department: Debt Collection')\n",
    "\n",
    "# Plotting histogram for  'mortgages_and_loans'\n",
    "sns.histplot(df[df['product'] == 'mortgages_and_loans']['num_words'], ax=axes[1, 0], color='green')\n",
    "axes[1, 0].set_title('Department: Mortgages and Loans')\n",
    "\n",
    "# Plotting histogram for'credit_card'\n",
    "sns.histplot(df[df['product'] == 'credit_card']['num_words'], ax=axes[1, 1], color='purple')\n",
    "axes[1, 1].set_title('Department: Credit Card')\n",
    "\n",
    "# Plotting histogram for 'retail_banking'\n",
    "sns.histplot(df[df['product'] == 'retail_banking']['num_words'], ax=axes[2, 0], color='purple')\n",
    "axes[2, 0].set_title('Department: Retail Banking')\n",
    "\n",
    "# An empty subplot for spacing\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "# Adjusting the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Showingthe subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "- The graph above illustrates that the word counts of the consumer complaints from all the department are right skewed. That is, most of the consumer complaints have very words counts.\n",
    "- It is important to note that there are outliers; that is, some consumers get into great depth when sending a consumer complaints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function of removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which stopwords are to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with stopwords removed.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Set of stopwords in English\n",
    "    tokens = text.split()                         # Splitting the text into individual words\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Filtering out stopwords\n",
    "    return ' '.join(filtered_tokens)              # Joining the filtered tokens back into a text\n",
    "\n",
    "#Removing stop words from training data\n",
    "df['updated_message'] = df['narrative'].apply(remove_stopwords)\n",
    "#Viewing the changes on the training dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing punctuations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function of removing puntuationa marks\n",
    "def remove_punctuation(post):\n",
    "    \"\"\"\n",
    "    Remove punctuation marks from the given post.\n",
    "\n",
    "    Args:\n",
    "        post (str): The input post from which punctuation marks are to be removed.\n",
    "\n",
    "    Returns:\n",
    "        str: The post with punctuation marks removed.\n",
    "    \"\"\"\n",
    "    return ''.join([l for l in post if l not in string.punctuation])\n",
    "\n",
    "# Create and Check if a new column contains messages with no punctuations\n",
    "df['updated_message'] = df['updated_message'].apply(remove_punctuation).str.lower() #Words converted to lower case\n",
    "#Viewing changes made on the training daraframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masking each department**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Reporting\n",
    "credit = df.loc[df['product'] == 'credit_reporting', 'updated_message']\n",
    "#Debt Collection\n",
    "debt = df.loc[df['product'] == 'debt_collection', 'updated_message']\n",
    "#Mortgages and loans\n",
    "mortgages = df.loc[df['product'] == 'mortgages_and_loans', 'updated_message']\n",
    "#Credit cards\n",
    "cards = df.loc[df['product'] == 'credit_card', 'updated_message']\n",
    "#Retail banking\n",
    "retail = df.loc[df['product'] == 'retail_banking', 'updated_message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Clouds for the five departments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the categories and their respective texts\n",
    "categories = ['credit reporting', 'debt collection', 'Mortgages and loans', 'credit cards', 'retail banking']\n",
    "texts = [credit, debt, mortgages, cards, retail]\n",
    "\n",
    "# Creating subplots\n",
    "fig, axs = plt.subplots(3, 2, figsize=(12, 9))\n",
    "fig.subplots_adjust(hspace=0)  # Adjusting the hspace parameter to remove vertical spacing\n",
    "\n",
    "# Generating word clouds for each category and plot them\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i < len(categories):  # Ensure we only use the first five categories\n",
    "        # Calculating word distribution\n",
    "        text = ' '.join(texts[i])\n",
    "        words = text.split()\n",
    "        print(f\"Category: {categories[i]}, Number of Words: {len(words)}\")  # Add this line to check the number of words\n",
    "\n",
    "        # Generating word cloud\n",
    "        wordcloud = WordCloud(max_words=20)\n",
    "        wordcloud.generate_from_frequencies(Counter(words))\n",
    "\n",
    "        # Ploting  the word cloud\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(categories[i])\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "# Showing the subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "    \n",
    "- Credit Reporting Complaints:\n",
    "The most common words in complaints related to credit reporting are \"Credit,\" \"account,\" \"reporting,\" \"report,\" and \"information.\" This suggests that consumers are often expressing concerns or frustrations related to their credit reports, accounts, and the accuracy or handling of credit-related information.\n",
    "\n",
    "- Debt Collection Complaints:\n",
    "The most common words in complaints related to debt collection are \"debt,\" \"credit,\" \"account,\" \"collection,\" and \"report.\" This indicates that consumers are frequently expressing issues with the collection of debts, interactions with debt collectors, and how these activities might impact their credit report.\n",
    "\n",
    "- Mortgage and Loan Complaints:\n",
    "The most common words in complaints related to mortgages and loans are \"Loan,\" \"payment,\" \"Mortgage,\" \"credit,\" and \"account.\" This suggests that consumers often have complaints concerning loan payments, mortgage-related issues, and credit aspects associated with loans.\n",
    "\n",
    "- Credit Card Complaints:\n",
    "The most common words in complaints related to credit cards are \"Credit,\" \"Card,\" \"account,\" \"payment,\" and \"charge.\" This indicates that consumers commonly express concerns regarding credit card transactions, account management, and charges they may have incurred.\n",
    "\n",
    "- Retail Banking Complaints:\n",
    "The most common words in complaints related to retail banking are \"account,\" \"bank,\" \"money,\" \"fund,\" and \"check.\" This suggests that consumers frequently express issues with their bank accounts, money transactions, funds management, and perhaps problems with check-related services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Resampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Reporting\n",
    "credit = df[df['product'] == 'credit_reporting']\n",
    "#Debt Collection\n",
    "debt = df[df['product'] == 'debt_collection']\n",
    "#Mortgages and loans\n",
    "mortgages = df[df['product'] == 'mortgages_and_loans']\n",
    "#Credit cards\n",
    "cards = df[df['product'] == 'credit_card']\n",
    "#Retail banking\n",
    "retail = df[df['product'] == 'retail_banking']\n",
    "\n",
    "\n",
    "\n",
    "Credit= resample(credit,\n",
    "                    replace=False, \n",
    "                    n_samples=13000, \n",
    "                    random_state=42)\n",
    "Debt = resample(debt,\n",
    "                   replace=False, \n",
    "                   n_samples=13000, \n",
    "                 random_state=42)\n",
    "Mortgages= resample(mortgages,\n",
    "                          replace=False,\n",
    "                          n_samples=13000, \n",
    "                          random_state=42)\n",
    "Cards= resample(cards,\n",
    "             replace=False, \n",
    "            n_samples=13000,\n",
    "            random_state=42)\n",
    "Retail= resample(retail,\n",
    "             replace=False, \n",
    "            n_samples=13000, \n",
    "            random_state=42)\n",
    "\n",
    "df = pd.concat([Credit, Debt, Mortgages, Cards, Retail])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    # Initializing the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenizing the text into individual words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize each word and join them back into a sentence\n",
    "    lemmatized_text = ' '.join(lemmatizer.lemmatize(word) for word in words)\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "#applying the function on the updated text\n",
    "df['lemmatized_message'] = df['updated_message'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(max_features=1000)\n",
    "X = tf.fit_transform(df['lemmatized_message']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_feature_names_out()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the response variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'response_variable' contains the class labels in string format\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['product'])\n",
    "y=y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "LR=LogisticRegression(max_iter=1000)\n",
    "#Random Forest Classifier\n",
    "RFC=RandomForestClassifier()\n",
    "#Gradient Boosting Classifier\n",
    "GBC=GradientBoostingClassifier()\n",
    "#Decision Tree classifier\n",
    "DTC=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "LR.fit(X_train, y_train)\n",
    "#Random Forest Classifier\n",
    "RFC.fit(X_train, y_train)\n",
    "#Gradient Boosting classifier\n",
    "GBC.fit(X_train, y_train)\n",
    "#Decision Tree Classifier\n",
    "DTC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting on the test and training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "LR_train=LR.predict(X_train)\n",
    "LR_test=LR.predict(X_test)\n",
    "#Random Forest Classifier\n",
    "RFC_train=RFC.predict(X_train)\n",
    "RFC_test=RFC.predict(X_test)\n",
    "#Gradient Boosting classifier\n",
    "GBC_train=GBC.predict(X_train)\n",
    "GBC_test=GBC.predict(X_test)\n",
    "#Decision Tree Classifier\n",
    "DTC_train=DTC.predict(X_train)\n",
    "DTC_test=DTC.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Comparing Model performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Explaining the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WORK IN PROGRESS....**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
